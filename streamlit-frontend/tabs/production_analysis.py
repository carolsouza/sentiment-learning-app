import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from typing import List, Dict, Any


@st.cache_data(ttl=300, show_spinner=False)
def _fetch_production_metrics(_api_client):
    """Busca m√©tricas do modelo de produ√ß√£o com cache de 5 minutos"""
    return _api_client.get_model_details("BiLSTM - Deep Learning")


def render_production_analysis_tab(api_client, api_status):
    """Renderiza a tab de Production Analysis - m√©tricas e an√°lise do modelo"""
    st.header("üöÄ Modelo de Produ√ß√£o - M√©tricas e An√°lise")

    # Apenas m√©tricas do modelo
    _render_api_metrics_production(api_client)


def _render_production_analysis_interface(api_client):
    """Interface simples para an√°lise de sentimento com modelo de produ√ß√£o"""

    # Informa√ß√£o sobre o modelo
    st.info("üöÄ **Modelo de Produ√ß√£o**: BiLSTM otimizado com dados balanceados para melhor performance.")

    # Se√ß√£o de teste individual
    st.subheader("üìù Teste de Texto")

    user_text = st.text_area(
        "Digite um texto para analisar:",
        placeholder="Ex: O atendimento foi excepcional e o produto chegou rapidamente!",
        height=100
    )

    if st.button("üöÄ Analisar com Modelo de Produ√ß√£o", use_container_width=True):
        if user_text.strip():
            with st.spinner("Analisando com modelo de produ√ß√£o..."):
                result = api_client.predict_production(user_text.strip())

            if result["success"]:
                prediction = result["prediction"]
                _display_production_result(prediction)
            else:
                st.error(f"‚ùå Erro: {result.get('error', 'Erro desconhecido')}")
        else:
            st.warning("‚ö†Ô∏è Digite um texto para an√°lise")

    # Exemplos r√°pidos
    st.markdown("---")
    st.subheader("üí° Exemplos R√°pidos")

    col1, col2 = st.columns(2)

    with col1:
        if st.button("üòÄ Exemplo Positivo"):
            _test_production_example(api_client, "O produto superou todas as minhas expectativas! Qualidade excepcional e entrega r√°pida.")

    with col2:
        if st.button("üòû Exemplo Negativo"):
            _test_production_example(api_client, "Produto de p√©ssima qualidade, n√£o funcionou e o suporte foi terr√≠vel.")


def _render_models_comparison(api_client):
    """Interface para compara√ß√£o entre modelos"""
    st.subheader("‚öñÔ∏è Compara√ß√£o: Baseline vs Produ√ß√£o")

    comparison_text = st.text_input(
        "Digite um texto para comparar os dois modelos:",
        placeholder="Ex: O produto √© bom mas o pre√ßo est√° alto"
    )

    if st.button("üîç Comparar Modelos", use_container_width=True):
        if comparison_text.strip():
            _compare_models(api_client, comparison_text.strip())
        else:
            st.warning("‚ö†Ô∏è Digite um texto para compara√ß√£o")



def _display_production_result(prediction):
    """Exibe resultado simples da predi√ß√£o do modelo de produ√ß√£o"""
    sentiment = prediction["sentiment"]
    score = prediction["score"]
    confidence = prediction["confidence"]

    # Resultado principal
    col1, col2, col3 = st.columns(3)

    with col1:
        emoji = "üòÄ" if sentiment == "positivo" else "üòû"
        st.metric(f"{emoji} Sentimento", sentiment.title())

    with col2:
        st.metric("üìä Score", f"{score:.3f}")

    with col3:
        st.metric("üéØ Confian√ßa", f"{confidence:.1%}")

    # Barra de progresso visual com indica√ß√£o de produ√ß√£o
    progress_color = "green" if sentiment == "positivo" else "red"
    st.markdown(f"""
    <div style="background-color: #f0f0f0; border-radius: 10px; padding: 10px; margin: 10px 0;">
        <div style="background-color: {progress_color}; height: 20px; width: {score*100}%; border-radius: 10px; text-align: center; color: white; font-weight: bold;">
            üöÄ Produ√ß√£o: {score:.3f}
        </div>
    </div>
    """, unsafe_allow_html=True)


def _test_production_example(api_client, text):
    """Testa um exemplo espec√≠fico com modelo de produ√ß√£o"""
    with st.spinner("Analisando exemplo com modelo de produ√ß√£o..."):
        result = api_client.predict_production(text)

    if result["success"]:
        st.write(f"**Texto:** {text}")
        _display_production_result(result["prediction"])
    else:
        st.error(f"‚ùå Erro: {result.get('error', 'Erro desconhecido')}")


def _compare_models(api_client, text):
    """Compara os resultados do modelo baseline e produ√ß√£o"""
    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ü§ñ Modelo Baseline")
        with st.spinner("Analisando com baseline..."):
            baseline_result = api_client.predict_baseline(text)

        if baseline_result["success"]:
            baseline_pred = baseline_result["prediction"]
            st.metric("Sentimento", baseline_pred["sentiment"].title())
            st.metric("Score", f"{baseline_pred['score']:.3f}")
            st.metric("Confian√ßa", f"{baseline_pred['confidence']:.1%}")
        else:
            st.error("‚ùå Erro no modelo baseline")

    with col2:
        st.subheader("üöÄ Modelo Produ√ß√£o")
        with st.spinner("Analisando com produ√ß√£o..."):
            prod_result = api_client.predict_production(text)

        if prod_result["success"]:
            prod_pred = prod_result["prediction"]
            st.metric("Sentimento", prod_pred["sentiment"].title())
            st.metric("Score", f"{prod_pred['score']:.3f}")
            st.metric("Confian√ßa", f"{prod_pred['confidence']:.1%}")
        else:
            st.error("‚ùå Erro no modelo de produ√ß√£o")

    # Compara√ß√£o visual se ambos funcionaram
    if baseline_result.get("success") and prod_result.get("success"):
        st.markdown("---")
        st.subheader("üìä Compara√ß√£o Visual")

        baseline_score = baseline_pred["score"]
        prod_score = prod_pred["score"]

        # Gr√°fico de barras comparativo
        fig = go.Figure(data=[
            go.Bar(name='Baseline', x=['Score'], y=[baseline_score], marker_color='lightblue'),
            go.Bar(name='Produ√ß√£o', x=['Score'], y=[prod_score], marker_color='darkgreen')
        ])

        fig.update_layout(
            title="Compara√ß√£o de Scores",
            yaxis_title="Score",
            yaxis=dict(range=[0, 1]),
            barmode='group',
            height=400
        )

        st.plotly_chart(fig, use_container_width=True)

        # Diferen√ßa
        diff = abs(prod_score - baseline_score)
        if diff > 0.1:
            if prod_score > baseline_score:
                st.success(f"üöÄ Modelo de produ√ß√£o mais positivo (+{diff:.3f})")
            else:
                st.info(f"ü§ñ Modelo baseline mais positivo (+{diff:.3f})")
        else:
            st.info(f"‚öñÔ∏è Modelos concordam (diferen√ßa: {diff:.3f})")


def _render_api_metrics_production(api_client):
    """Renderiza m√©tricas do modelo de produ√ß√£o via API"""
    col1, col2 = st.columns([3, 1])
    with col1:
        st.subheader("üìä M√©tricas do Modelo de Produ√ß√£o")
    with col2:
        if st.button("üîÑ Atualizar", key="refresh_production"):
            _fetch_production_metrics.clear()
            st.rerun()

    with st.spinner("üîÑ Carregando m√©tricas..."):
        result = _fetch_production_metrics(api_client)

    if not result["success"]:
        st.error(f"‚ùå {result.get('error', 'Erro ao carregar m√©tricas')}")
        return

    data = result["data"]

    if not data.get("found", False):
        st.warning("üîç Modelo 'BiLSTM - Deep Learning' n√£o encontrado no MLflow")
        return

    # Info do modelo
    model_info = data["model_info"]
    st.success(f"‚úÖ Modelo: **{model_info['model_name']}** (v{model_info['version']}) - Stage: {model_info['stage']}")

    # M√©tricas principais - Test
    st.markdown("### üéØ M√©tricas de Teste")
    metrics = data["metrics"]

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("F1-Score", f"{metrics.get('test_f1', 0):.4f}")
    with col2:
        st.metric("Accuracy", f"{metrics.get('test_accuracy', 0):.4f}")
    with col3:
        st.metric("ROC-AUC", f"{metrics.get('test_roc_auc', 0):.4f}")
    with col4:
        st.metric("Threshold", f"{metrics.get('best_threshold', 0.5):.3f}")

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Precision", f"{metrics.get('test_precision', 0):.4f}")
    with col2:
        st.metric("Recall", f"{metrics.get('test_recall', 0):.4f}")
    with col3:
        if metrics.get('best_epoch'):
            st.metric("Best Epoch", f"{int(metrics.get('best_epoch', 0))}")
    with col4:
        if metrics.get('best_f1_val'):
            st.metric("Best F1 (Val)", f"{metrics.get('best_f1_val', 0):.4f}")

    # Dataset Info
    dataset = data.get("dataset_info", {})
    if dataset:
        st.markdown("### üì¶ Dataset")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Train", f"{int(dataset.get('train_size', 0)):,}")
        with col2:
            st.metric("Validation", f"{int(dataset.get('val_size', 0)):,}")
        with col3:
            st.metric("Test", f"{int(dataset.get('test_size', 0)):,}")

    # Par√¢metros
    st.markdown("---")
    st.markdown("### üîß Hiperpar√¢metros")
    params = data.get("params", {})

    params_col1, params_col2, params_col3 = st.columns(3)

    with params_col1:
        st.write("**Arquitetura:**")
        for key in ['model_type', 'lstm_units', 'vocab_size', 'embed_dim']:
            if key in params:
                st.write(f"‚Ä¢ {key}: {params[key]}")

    with params_col2:
        st.write("**Treinamento:**")
        for key in ['optimizer', 'learning_rate', 'batch_size', 'clipnorm']:
            if key in params:
                st.write(f"‚Ä¢ {key}: {params[key]}")

    with params_col3:
        st.write("**Outros:**")
        for key in ['dropout_rate', 'recurrent_dropout', 'total_params']:
            if key in params:
                st.write(f"‚Ä¢ {key}: {params[key]}")

    # Gr√°ficos
    st.markdown("---")
    st.markdown("### üìä Visualiza√ß√µes")

    training_history = data.get("training_history", {})
    cm = data.get("confusion_matrix", {})

    # Verificar se h√° dados para mostrar
    has_loss = training_history.get("loss") and training_history.get("val_loss")
    has_auc = training_history.get("auc") and training_history.get("val_auc")
    has_accuracy = training_history.get("accuracy") and training_history.get("val_accuracy")
    has_precision = training_history.get("precision") and training_history.get("val_precision")
    has_recall = training_history.get("recall") and training_history.get("val_recall")
    has_confusion_matrix = cm.get('tn') is not None or cm.get('tp') is not None

    if has_loss or has_auc or has_accuracy or has_precision or has_recall or has_confusion_matrix:
        # Criar subplot com 3 linhas x 2 colunas
        fig = make_subplots(
            rows=3, cols=2,
            subplot_titles=("Loss", "AUC", "Accuracy", "Precision", "Recall", "Confusion Matrix"),
            specs=[
                [{"type": "scatter"}, {"type": "scatter"}],
                [{"type": "scatter"}, {"type": "scatter"}],
                [{"type": "scatter"}, {"type": "heatmap"}]
            ]
        )

        # Gr√°fico de Loss
        if has_loss:
            loss_data = training_history["loss"]
            val_loss_data = training_history["val_loss"]

            epochs = [item["epoch"] for item in loss_data]
            values = [item["value"] for item in loss_data]
            fig.add_trace(go.Scatter(x=epochs, y=values,
                                   mode='lines', name='Train Loss',
                                   line=dict(color='blue'), showlegend=True), row=1, col=1)

            epochs = [item["epoch"] for item in val_loss_data]
            values = [item["value"] for item in val_loss_data]
            fig.add_trace(go.Scatter(x=epochs, y=values,
                                   mode='lines', name='Val Loss',
                                   line=dict(color='red'), showlegend=True), row=1, col=1)

        # Gr√°fico de AUC
        if has_auc:
            auc_data = training_history["auc"]
            val_auc_data = training_history["val_auc"]

            epochs = [item["epoch"] for item in auc_data]
            values = [item["value"] for item in auc_data]
            fig.add_trace(go.Scatter(x=epochs, y=values,
                                   mode='lines', name='Train AUC',
                                   line=dict(color='green'), showlegend=False), row=1, col=2)

            epochs = [item["epoch"] for item in val_auc_data]
            values = [item["value"] for item in val_auc_data]
            fig.add_trace(go.Scatter(x=epochs, y=values,
                                   mode='lines', name='Val AUC',
                                   line=dict(color='orange'), showlegend=False), row=1, col=2)

        # Gr√°fico de Accuracy
        if has_accuracy:
            acc_data = training_history["accuracy"]
            val_acc_data = training_history["val_accuracy"]

            epochs = [item["epoch"] for item in acc_data]
            values = [item["value"] for item in acc_data]
            fig.add_trace(go.Scatter(x=epochs, y=values,
                                   mode='lines', name='Train Acc',
                                   line=dict(color='purple'), showlegend=False), row=2, col=1)

            epochs = [item["epoch"] for item in val_acc_data]
            values = [item["value"] for item in val_acc_data]
            fig.add_trace(go.Scatter(x=epochs, y=values,
                                   mode='lines', name='Val Acc',
                                   line=dict(color='brown'), showlegend=False), row=2, col=1)

        # Gr√°fico de Precision
        if has_precision:
            prec_data = training_history["precision"]
            val_prec_data = training_history["val_precision"]

            epochs = [item["epoch"] for item in prec_data]
            values = [item["value"] for item in prec_data]
            fig.add_trace(go.Scatter(x=epochs, y=values,
                                   mode='lines', name='Train Precision',
                                   line=dict(color='cyan'), showlegend=False), row=2, col=2)

            epochs = [item["epoch"] for item in val_prec_data]
            values = [item["value"] for item in val_prec_data]
            fig.add_trace(go.Scatter(x=epochs, y=values,
                                   mode='lines', name='Val Precision',
                                   line=dict(color='magenta'), showlegend=False), row=2, col=2)

        # Gr√°fico de Recall
        if has_recall:
            rec_data = training_history["recall"]
            val_rec_data = training_history["val_recall"]

            epochs = [item["epoch"] for item in rec_data]
            values = [item["value"] for item in rec_data]
            fig.add_trace(go.Scatter(x=epochs, y=values,
                                   mode='lines', name='Train Recall',
                                   line=dict(color='yellow'), showlegend=False), row=3, col=1)

            epochs = [item["epoch"] for item in val_rec_data]
            values = [item["value"] for item in val_rec_data]
            fig.add_trace(go.Scatter(x=epochs, y=values,
                                   mode='lines', name='Val Recall',
                                   line=dict(color='pink'), showlegend=False), row=3, col=1)

        # Confusion Matrix
        if has_confusion_matrix:
            confusion_matrix = [
                [cm.get('tn', 0), cm.get('fp', 0)],
                [cm.get('fn', 0), cm.get('tp', 0)]
            ]

            fig.add_trace(go.Heatmap(
                z=confusion_matrix,
                x=['Negative', 'Positive'],
                y=['Negative', 'Positive'],
                text=confusion_matrix,
                texttemplate='%{text}',
                textfont={"size": 16},
                colorscale='Blues',
                showscale=False
            ), row=3, col=2)

        fig.update_xaxes(title_text="√âpoca", row=1, col=1)
        fig.update_xaxes(title_text="√âpoca", row=1, col=2)
        fig.update_xaxes(title_text="√âpoca", row=2, col=1)
        fig.update_xaxes(title_text="√âpoca", row=2, col=2)
        fig.update_xaxes(title_text="√âpoca", row=3, col=1)
        fig.update_xaxes(title_text="Predicted", row=3, col=2)
        fig.update_yaxes(title_text="Actual", row=3, col=2)

        fig.update_layout(
            height=900,
            showlegend=True,
            hovermode='x unified'
        )

        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("üìä Dados de visualiza√ß√£o n√£o dispon√≠veis")

    # Link MLflow
    st.markdown("---")
    st.info(f"üîó [Ver no MLflow]({model_info['mlflow_uri']})")


def _render_api_models_comparison(api_client):
    """Renderiza compara√ß√£o entre modelos via API"""
    st.subheader("‚öñÔ∏è Compara√ß√£o: Baseline vs Produ√ß√£o")

    with st.spinner("üîÑ Carregando compara√ß√£o entre modelos..."):
        result = api_client.get_model_comparison()

    if not result["success"]:
        st.error(f"‚ùå Erro ao carregar compara√ß√£o: {result.get('error', 'Erro desconhecido')}")
        return

    data = result["data"]

    if not data.get("comparison_available", False):
        st.warning("üîç Compara√ß√£o n√£o dispon√≠vel")
        st.info(f"Baseline encontrado: {'‚úÖ' if data.get('baseline_found', False) else '‚ùå'}")
        st.info(f"Produ√ß√£o encontrado: {'‚úÖ' if data.get('production_found', False) else '‚ùå'}")
        return

    # Resumo da compara√ß√£o
    summary = data["summary"]
    col1, col2, col3 = st.columns(3)

    with col1:
        st.metric("üèÜ Vit√≥rias Produ√ß√£o", summary["production_wins"])

    with col2:
        st.metric("ü•à Vit√≥rias Baseline", summary["baseline_wins"])

    with col3:
        st.metric("ü§ù Empates", summary["ties"])

    # Tabela de compara√ß√£o
    st.markdown("---")
    st.subheader("üìä M√©tricas Detalhadas")

    comparison_data = data["comparison"]

    # Prepara dados para a tabela
    table_data = []
    for item in comparison_data:
        table_data.append({
            "M√©trica": item["metric"].replace("_", " ").title(),
            "Baseline": f"{item['baseline']:.4f}",
            "Produ√ß√£o": f"{item['production']:.4f}",
            "Diferen√ßa": f"{item['difference']:+.4f}",
            "Melhor": item["better_model"].title()
        })

    df_comparison = pd.DataFrame(table_data)
    st.dataframe(df_comparison, use_container_width=True, hide_index=True)

    # Gr√°fico comparativo
    st.markdown("---")
    st.subheader("üìà Visualiza√ß√£o Comparativa")

    # Seleciona m√©tricas principais para o gr√°fico
    metrics_for_chart = ["test_f1", "test_roc_auc"]
    baseline_values = []
    production_values = []
    metric_names = []

    for item in comparison_data:
        if item["metric"] in metrics_for_chart:
            baseline_values.append(item["baseline"])
            production_values.append(item["production"])
            metric_names.append(item["metric"].replace("_", " ").title())

    if baseline_values and production_values:
        fig = go.Figure(data=[
            go.Bar(name='Baseline (DNN)', x=metric_names, y=baseline_values, marker_color='lightblue'),
            go.Bar(name='Produ√ß√£o (BiLSTM)', x=metric_names, y=production_values, marker_color='darkgreen')
        ])

        fig.update_layout(
            title="Compara√ß√£o de Performance: Baseline vs Produ√ß√£o",
            yaxis_title="Score",
            yaxis=dict(range=[0, 1]),
            barmode='group',
            height=400
        )

        st.plotly_chart(fig, use_container_width=True)

    # Conclus√£o
    st.markdown("---")
    if summary["production_wins"] > summary["baseline_wins"]:
        st.success("üöÄ **Modelo de Produ√ß√£o (BiLSTM)** apresenta melhor performance geral!")
    elif summary["baseline_wins"] > summary["production_wins"]:
        st.info("ü§ñ **Modelo Baseline (DNN)** apresenta melhor performance geral!")
    else:
        st.warning("‚öñÔ∏è **Modelos empatados** - Performance similar entre os dois modelos!")


def _render_old_mlflow_metrics_production():
    """Renderiza m√©tricas do modelo de produ√ß√£o do MLflow"""
    st.subheader("üìä M√©tricas do Modelo de Produ√ß√£o (BiLSTM)")

    try:
        # Busca experimentos do modelo de produ√ß√£o
        client = mlflow.tracking.MlflowClient()

        # Busca runs com o nome "production_bilstm64_dnn"
        runs = client.search_runs(
            experiment_ids=["0"],  # Default experiment
            filter_string="run_name = 'production_bilstm64_dnn'",
            order_by=["start_time DESC"],
            max_results=5
        )

        if not runs:
            st.warning("üîç Nenhum experimento de produ√ß√£o encontrado no MLflow")
            st.info("Execute o script de treino de produ√ß√£o primeiro:\n```\ncd models-training\npython train_production.py\n```")
            return

        # Pega o run mais recente
        latest_run = runs[0]
        run_id = latest_run.info.run_id

        st.success(f"‚úÖ Experimento encontrado: {run_id[:8]}...")

        # M√©tricas principais
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            test_f1 = latest_run.data.metrics.get("test_f1", 0)
            st.metric("üéØ F1-Score (Test)", f"{test_f1:.4f}")

        with col2:
            test_roc_auc = latest_run.data.metrics.get("test_roc_auc", 0)
            st.metric("üìà ROC-AUC (Test)", f"{test_roc_auc:.4f}")

        with col3:
            val_loss = latest_run.data.metrics.get("val_loss", 0)
            st.metric("üìâ Validation Loss", f"{val_loss:.4f}")

        with col4:
            best_threshold = latest_run.data.metrics.get("best_threshold", 0.5)
            st.metric("‚öñÔ∏è Best Threshold", f"{best_threshold:.3f}")

        # Par√¢metros do modelo
        st.markdown("---")
        st.subheader("üîß Par√¢metros do Modelo")

        params_col1, params_col2, params_col3 = st.columns(3)

        with params_col1:
            st.write("**Arquitetura:**")
            st.write(f"‚Ä¢ Tipo: {latest_run.data.params.get('model_type', 'N/A')}")
            st.write(f"‚Ä¢ LSTM Units: {latest_run.data.params.get('lstm_units', 'N/A')}")
            st.write(f"‚Ä¢ Vocab Size: {latest_run.data.params.get('vocab_size', 'N/A')}")
            st.write(f"‚Ä¢ Embed Dim: {latest_run.data.params.get('embed_dim', 'N/A')}")

        with params_col2:
            st.write("**Treinamento:**")
            st.write(f"‚Ä¢ Optimizer: {latest_run.data.params.get('optimizer', 'N/A')}")
            st.write(f"‚Ä¢ Learning Rate: {latest_run.data.params.get('learning_rate', 'N/A')}")
            st.write(f"‚Ä¢ Batch Size: {latest_run.data.params.get('batch_size', 'N/A')}")
            st.write(f"‚Ä¢ Clipnorm: {latest_run.data.params.get('clipnorm', 'N/A')}")

        with params_col3:
            st.write("**Regulariza√ß√£o:**")
            st.write(f"‚Ä¢ Dropout: {latest_run.data.params.get('dropout_rate', 'N/A')}")
            st.write(f"‚Ä¢ Recurrent Dropout: {latest_run.data.params.get('recurrent_dropout', 'N/A')}")
            st.write(f"‚Ä¢ Total Params: {latest_run.data.params.get('total_params', 'N/A')}")

        # Hist√≥rico de m√©tricas (se dispon√≠vel)
        st.markdown("---")
        st.subheader("üìà Hist√≥rico de Treinamento")

        try:
            # Busca hist√≥rico de m√©tricas
            metric_history = client.get_metric_history(run_id, "loss")
            val_metric_history = client.get_metric_history(run_id, "val_loss")

            if metric_history and val_metric_history:
                # Cria DataFrame para plotar
                train_data = pd.DataFrame([(m.step, m.value) for m in metric_history], columns=['epoch', 'train_loss'])
                val_data = pd.DataFrame([(m.step, m.value) for m in val_metric_history], columns=['epoch', 'val_loss'])

                # Gr√°fico de loss
                fig = go.Figure()
                fig.add_trace(go.Scatter(x=train_data['epoch'], y=train_data['train_loss'],
                                       mode='lines', name='Training Loss', line=dict(color='blue')))
                fig.add_trace(go.Scatter(x=val_data['epoch'], y=val_data['val_loss'],
                                       mode='lines', name='Validation Loss', line=dict(color='red')))

                fig.update_layout(
                    title="Evolu√ß√£o do Loss durante o Treinamento (BiLSTM)",
                    xaxis_title="√âpoca",
                    yaxis_title="Loss",
                    hovermode='x unified'
                )

                st.plotly_chart(fig, use_container_width=True)

                # Busca outras m√©tricas se dispon√≠veis
                try:
                    auc_history = client.get_metric_history(run_id, "val_auc")
                    val_auc_history = client.get_metric_history(run_id, "val_auc")

                    if auc_history:
                        auc_data = pd.DataFrame([(m.step, m.value) for m in auc_history], columns=['epoch', 'val_auc'])

                        # Gr√°fico de AUC
                        fig_auc = go.Figure()
                        fig_auc.add_trace(go.Scatter(x=auc_data['epoch'], y=auc_data['val_auc'],
                                                   mode='lines', name='Validation AUC', line=dict(color='green')))

                        fig_auc.update_layout(
                            title="Evolu√ß√£o do AUC durante o Treinamento",
                            xaxis_title="√âpoca",
                            yaxis_title="AUC",
                            hovermode='x unified'
                        )

                        st.plotly_chart(fig_auc, use_container_width=True)
                except:
                    pass  # AUC pode n√£o estar dispon√≠vel

            else:
                st.info("üìä Hist√≥rico de m√©tricas n√£o dispon√≠vel para este run")

        except Exception as e:
            st.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel carregar o hist√≥rico: {str(e)}")

        # Compara√ß√£o com baseline (se dispon√≠vel)
        st.markdown("---")
        st.subheader("‚öñÔ∏è Compara√ß√£o com Baseline")

        try:
            # Busca tamb√©m o baseline para comparar
            baseline_runs = client.search_runs(
                experiment_ids=["0"],
                filter_string="run_name = 'baseline_dnn_pool'",
                order_by=["start_time DESC"],
                max_results=1
            )

            if baseline_runs:
                baseline_run = baseline_runs[0]

                # Tabela comparativa
                comparison_data = {
                    "M√©trica": ["F1-Score (Test)", "ROC-AUC (Test)", "Validation Loss", "Best Threshold"],
                    "Baseline (DNN)": [
                        f"{baseline_run.data.metrics.get('test_f1', 0):.4f}",
                        f"{baseline_run.data.metrics.get('test_roc_auc', 0):.4f}",
                        f"{baseline_run.data.metrics.get('val_loss', 0):.4f}",
                        f"{baseline_run.data.metrics.get('best_threshold', 0.5):.3f}"
                    ],
                    "Produ√ß√£o (BiLSTM)": [
                        f"{latest_run.data.metrics.get('test_f1', 0):.4f}",
                        f"{latest_run.data.metrics.get('test_roc_auc', 0):.4f}",
                        f"{latest_run.data.metrics.get('val_loss', 0):.4f}",
                        f"{latest_run.data.metrics.get('best_threshold', 0.5):.3f}"
                    ]
                }

                df_comparison = pd.DataFrame(comparison_data)
                st.dataframe(df_comparison, use_container_width=True, hide_index=True)

                # Gr√°fico comparativo
                fig_comp = go.Figure(data=[
                    go.Bar(name='Baseline (DNN)', x=['F1-Score', 'ROC-AUC'],
                          y=[baseline_run.data.metrics.get('test_f1', 0),
                             baseline_run.data.metrics.get('test_roc_auc', 0)],
                          marker_color='lightblue'),
                    go.Bar(name='Produ√ß√£o (BiLSTM)', x=['F1-Score', 'ROC-AUC'],
                          y=[latest_run.data.metrics.get('test_f1', 0),
                             latest_run.data.metrics.get('test_roc_auc', 0)],
                          marker_color='darkgreen')
                ])

                fig_comp.update_layout(
                    title="Compara√ß√£o de Performance: Baseline vs Produ√ß√£o",
                    yaxis_title="Score",
                    yaxis=dict(range=[0, 1]),
                    barmode='group',
                    height=400
                )

                st.plotly_chart(fig_comp, use_container_width=True)

            else:
                st.info("üîç Baseline n√£o encontrado para compara√ß√£o")

        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erro na compara√ß√£o: {str(e)}")

        # Informa√ß√µes do experimento
        st.markdown("---")
        st.subheader("‚ÑπÔ∏è Informa√ß√µes do Experimento")

        info_col1, info_col2 = st.columns(2)

        with info_col1:
            st.write(f"**Run ID:** `{run_id}`")
            st.write(f"**Status:** {latest_run.info.status}")
            st.write(f"**In√≠cio:** {pd.to_datetime(latest_run.info.start_time, unit='ms').strftime('%Y-%m-%d %H:%M:%S')}")

        with info_col2:
            if latest_run.info.end_time:
                end_time = pd.to_datetime(latest_run.info.end_time, unit='ms')
                start_time = pd.to_datetime(latest_run.info.start_time, unit='ms')
                duration = end_time - start_time
                st.write(f"**Fim:** {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
                st.write(f"**Dura√ß√£o:** {duration}")

        # Link para MLflow UI
        st.markdown("---")
        st.info(f"üîó [Ver experimento completo no MLflow](https://mlflow-server-273169854208.us-central1.run.app/#/experiments/0/runs/{run_id})")

    except Exception as e:
        st.error(f"‚ùå Erro ao conectar com MLflow: {str(e)}")
        st.info("Verifique se o MLflow server est√° acess√≠vel em: https://mlflow-server-273169854208.us-central1.run.app")


